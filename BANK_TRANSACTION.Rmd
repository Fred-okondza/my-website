---
title: "Bank"
author: "okondza_Fred"
date: "2025-04-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(dplyr)
library(tidyr)
library(e1071)
library(randomForest)
library(car)
library(corrplot)
library(ggplot2)
library(lubridate)
library(data.table)
library(caret)
library(xgboost)
library(lightgbm)
library(keras)
library(ROCR)
library(MLmetrics)
library(tidyverse)
library(viridis)
library(forcats)
library(gridExtra)
library(reshape2)  # For melting the correlation matrix
library(glmnet)
```



# Bank Transactions Analysis


```{r}
set.seed(42)  # Set seed for reproducibility
df <- read.csv("/home/student24/Coop_Projects/bank_transactions_data_2.csv", stringsAsFactors = FALSE)
head(df)
```




```{r}
str(df)  # Display the structure of the dataset
```



```{r}
# Convert specified columns to factors
df$Location <- as.factor(df$Location)
df$DeviceID <- as.factor(df$DeviceID)
df$Channel <- as.factor(df$Channel)
df$CustomerOccupation <- as.factor(df$CustomerOccupation)
df$MerchantID <- as.factor(df$MerchantID)
```



```{r}
summary(df)  # Get descriptive statistics for the dataset
```

# Check the missing values

```{r}
colnames(df)  # Display the names of the columns
```



```{r}
missing_values <- colSums(is.na(df))
print(missing_values)  # Check for missing values in the dataset
dimensions <- dim(df)
print(dimensions)  # Print the number of rows and columns
```


```{r}
# Create a summary table of unique counts for each column
unique_counts_table <- data.frame(
  Column = names(df),
  Unique_Count = sapply(df, function(x) length(unique(x)))  # Change 'bank' to 'df'
)

# Print the table
print(unique_counts_table)
```


# Step 3: Data Cleaning and Preprocessing Convert Dates to Datetime Format

```{r}
df$TransactionDate <- as.POSIXct(df$TransactionDate, format="%Y-%m-%d %H:%M:%S")
df$PreviousTransactionDate <- as.POSIXct(df$PreviousTransactionDate, format="%Y-%m-%d %H:%M:%S")
```


```{r}
summary(df)
```


```{r}
print(unique(df$TransactionType))  # Vérifie les valeurs actuelles
print(levels(factor(df$TransactionType)))
```



# 2. Label Encoding for Categorical Variables:

```{r}
df$TransactionType <- as.numeric(factor(df$TransactionType)) - 1
                            
```


# 3. Feature Engineering

```{r}
# Calculate the number of days between transactions
df$TimeSinceLastTransaction <- as.numeric(difftime(df$TransactionDate, df$PreviousTransactionDate, units = "days"))
```


# 4. Standardize Numerical Features:

```{r}
# Define the numerical columns
numerical_cols <- c("TransactionAmount", "TransactionDuration", "AccountBalance", "CustomerAge")

# Standardize (mean = 0, standard deviation = 1)
df[numerical_cols] <- scale(df[numerical_cols])
```



# Step 4: Exploratory Data Analysis (EDA)

```{r}
# Load necessary libraries
library(ggplot2)
library(gridExtra)

# Create histograms for each numerical column
plots <- lapply(numerical_cols, function(col) {
  ggplot(df, aes(x = .data[[col]])) +  # Use .data[[col]] instead of aes_string(x = col)
    geom_histogram(bins = 15, fill = "blue", color = "black", alpha = 0.7) +
    labs(title = paste("Histogram of", col), x = col, y = "Count") +
    theme_minimal()
})

# Arrange plots in a 2x2 layout
grid.arrange(grobs = plots, nrow = 2, ncol = 2)
```



# Transaction Amount Distribution


```{r}
# Création de l'histogramme avec courbe de densité
ggplot(df, aes(x = TransactionAmount)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "blue", color = "black", alpha = 0.7) +
  geom_density(color = "red", size = 1) +  # Ajout de la courbe KDE (Kernel Density Estimation)
  ggtitle("Distribution of Transaction Amounts") +
  xlab("Transaction Amount") +
  ylab("Density") +
  theme_minimal()

```


# Daily Transaction Count (Time-based Analysis)¶


```{r}
# Charger les bibliothèques nécessaires
library(lubridate)

# Convertir TransactionDate en format Date
df$TransactionDate <- as.Date(df$TransactionDate)  

# Extraire uniquement la date (sans l'heure)
df$TransactionDay <- as.Date(df$TransactionDate)

# Compter les transactions par jour
daily_counts <- df %>%
  group_by(TransactionDay) %>%
  summarise(TransactionCount = n())

# Tracer la courbe des transactions quotidiennes
ggplot(daily_counts, aes(x = TransactionDay, y = TransactionCount)) +
  geom_line(color = "red", size = 1) +
  ggtitle("Daily Transaction Counts") +
  xlab("Date") +
  ylab("Transaction Count") +
  theme_minimal()

```

# Transaction Count by Day of the Week

```{r}
# Convert TransactionDate to Date (ensure it's in Date format)
df$TransactionDate <- as.Date(df$TransactionDate)

# Extract the day of the week
df$DayOfWeek <- weekdays(df$TransactionDate)

# Reorder days of the week in the desired order
df$DayOfWeek <- factor(df$DayOfWeek, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))

# Define easily interpretable colors
day_colors <- c(
  "Monday" = "#FF9999",     # Light Red
  "Tuesday" = "#FFCC99",    # Light Orange
  "Wednesday" = "#FFFF99",  # Light Yellow
  "Thursday" = "#99FF99",   # Light Green
  "Friday" = "#99CCFF",     # Light Blue
  "Saturday" = "#CC99FF",   # Light Purple
  "Sunday" = "#FFCCFF"      # Light Pink
)

# Plot with distinct, easy-to-interpret colors
ggplot(df, aes(x = DayOfWeek, fill = DayOfWeek)) +
  geom_bar(color = "black") +
  scale_fill_manual(values = day_colors) +  # Applying the custom colors
  ggtitle("Transaction Count by Day of the Week") +
  xlab("Day of the Week") +
  ylab("Number of Transactions") +
  theme_minimal()
```



# Top Locations by Transaction Volume


```{r}
library(forcats)

# Get the top 10 locations by transaction volume
top_locations <- df %>%
  count(Location) %>%
  top_n(10, n) %>%
  arrange(desc(n))

# Plot top 10 locations by transaction volume
ggplot(top_locations, aes(x = n, y = fct_reorder(Location, n), fill = n)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_fill_viridis_c() +  # Use a color palette (coolwarm equivalent)
  ggtitle("Top 10 Locations by Transaction Volume") +
  xlab("Number of Transactions") +
  ylab("Location") +
  theme_minimal()

```


# Categorical Analysis: Transaction type counts

```{r}
# Categorical Analysis: Transaction type counts

# Create a bar plot for the distribution of 'TransactionType'
ggplot(df, aes(x = TransactionType)) +
  geom_bar(fill = "blue", color = "black", alpha = 0.7) +  # Bar chart for counts
  labs(title = "Transaction Type Distribution", x = "Transaction Type", y = "Count") +
  theme_minimal()
```



# Bivariate Analysis: Transaction amount by channel

```{r}
# Create a box plot with three colors
ggplot(df, aes(x = factor(Channel), y = TransactionAmount, fill = factor(Channel))) +
  geom_boxplot(color = "black", alpha = 0.7) +
  scale_fill_manual(values = c("blue", "orange", "green")) +
  labs(title = "Transaction Amount by Channel", x = "Channel", y = "Transaction Amount") +
  theme_minimal()
```

# Correlation Heatmap: Correlation between numerical features

```{r}
# Compute correlation matrix
cor_matrix <- cor(df[, c(numerical_cols, "TransactionType")], use = "complete.obs")

# Create a heatmap
corrplot(cor_matrix, method = "color", type = "upper", tl.col = "black", 
         tl.srt = 45, col = colorRampPalette(c("blue", "white", "red"))(200),
         title = "Correlation Matrix", mar = c(0, 0, 2, 0))
```
# Step 5: Feature Selection.

```{r}
# Define the feature matrix X
X <- df[, c("TransactionAmount", "Channel", "CustomerAge", 
            "CustomerOccupation", "TransactionDuration", 
            "LoginAttempts", "AccountBalance", "TimeSinceLastTransaction")]

# Define the target variable y
y <- df$TransactionType
```


# Step 6: Train-Test Split.

```{r}
# Set a random seed for reproducibility
set.seed(42)

# Split the data (70% training, 30% testing)
train_index <- createDataPartition(y, p = 0.7, list = FALSE)

# Training set
X_train <- X[train_index, ]
y_train <- y[train_index]

# Testing set
X_test <- X[-train_index, ]
y_test <- y[-train_index]
```


# Step 7: Model Building.
# 1. Logistic Regression Model:

```{r}
# Fit a logistic regression model
log_reg <- glm(y_train ~ ., data = data.frame(X_train), family = "binomial")
```


# 2. Random Forest Classifier

```{r}
# Train a Random Forest model
set.seed(42)  # Ensure reproducibility
rf_model <- randomForest(x = X_train, y = as.factor(y_train), ntree = 100)
```



```{r}
# Step 8: Model Evaluation
# 1. Accuracy and Classification Report

# Predictions with Logistic Regression
y_pred_log <- predict(log_reg, newdata = data.frame(X_test), type = "response")
y_pred_log_binary <- ifelse(y_pred_log > 0.5, 1, 0)

# Convert predictions to factor ensuring consistent levels
y_pred_log_binary <- factor(y_pred_log_binary, levels = c(0, 1))

# Create a confusion matrix for Logistic Regression
log_reg_cm <- confusionMatrix(y_pred_log_binary, factor(y_test, levels = c(0, 1)))
print("Logistic Regression Classification Report:")
print(log_reg_cm)
cat("Logistic Regression Accuracy:", log_reg_cm$overall['Accuracy'], "\n")

# Predictions with Random Forest
y_pred_rf <- predict(rf_model, newdata = X_test)

# Convert predictions to factor ensuring consistent levels
y_pred_rf <- factor(y_pred_rf, levels = c(0, 1))

# Create a confusion matrix for Random Forest
rf_cm <- confusionMatrix(y_pred_rf, factor(y_test, levels = c(0, 1)))
print("Random Forest Classification Report:")
print(rf_cm)
cat("Random Forest Accuracy:", rf_cm$overall['Accuracy'], "\n")
```



# 2. Confusion Matrix


```{r}

# Fonction pour tracer la matrice de confusion
plot_confusion_matrix <- function(cm, title) {
  cm_melt <- melt(cm)  # Reshape confusion matrix for ggplot
  colnames(cm_melt) <- c("Actual", "Predicted", "Count")  # Rename columns
  
  ggplot(cm_melt, aes(x = Predicted, y = Actual, fill = Count)) + 
    geom_tile(color = "white") + 
    scale_fill_gradient(low = "white", high = "blue") +
    geom_text(aes(label = Count), color = "black") +
    labs(title = title, x = "Predicted", y = "Actual") +
    theme_minimal()
}

# Créer la matrice de confusion pour la régression logistique
log_reg_cm <- confusionMatrix(y_pred_log_binary, factor(y_test, levels = c(0, 1)))  # Assurez-vous que y_test est un facteur
conf_matrix_log <- log_reg_cm$table
plot_confusion_matrix(conf_matrix_log, "Logistic Regression Confusion Matrix")

# Créer la matrice de confusion pour la forêt aléatoire
rf_cm <- confusionMatrix(y_pred_rf, factor(y_test, levels = c(0, 1)))  # Assurez-vous que y_test est un facteur
conf_matrix_rf <- rf_cm$table
plot_confusion_matrix(conf_matrix_rf, "Random Forest Confusion Matrix")
```



# 3. ROC and AUC Curve

```{r}
# Compute predicted probabilities for Random Forest
y_pred_prob_rf <- predict(rf_model, X_test, type = "prob")[, 2]

# Compute ROC curve and AUC
roc_obj <- roc(y_test, y_pred_prob_rf)
roc_auc <- auc(roc_obj)

# Convert ROC data to a dataframe for ggplot2
roc_df <- data.frame(FPR = 1 - roc_obj$specificities, 
                      TPR = roc_obj$sensitivities)

# Plot ROC Curve
ggplot(roc_df, aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "ROC Curve", 
       x = "False Positive Rate", 
       y = "True Positive Rate") +
  annotate("text", x = 0.6, y = 0.2, 
           label = sprintf("AUC = %.2f", roc_auc), 
           color = "blue", size = 5) +
  theme_minimal()
```



# Step 9: Cross-Validation

```{r}
# Create a control function for cross-validation
train_control <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Perform cross-validation for Random Forest
cv_model <- train(x = X, y = as.factor(y), method = "rf", trControl = train_control)

# Extract cross-validation scores
cv_scores_rf <- cv_model$resample$Accuracy  # Get accuracy scores from resampling results

# Display results
cat("Random Forest Cross-Validation Scores:", cv_scores_rf, "\n")
cat("Random Forest Mean CV Score:", mean(cv_scores_rf), "\n")
```



# Step 10: Feature Importance (for Random Forest)

```{r}
# Calculate feature importances
feature_importances <- importance(rf_model)  # Get feature importances
feature_importances <- feature_importances[, 1]  # Select only the importance values

# Get sorted indices based on feature importance
sorted_indices <- order(feature_importances, decreasing = TRUE)

# Create a data frame for plotting
importances_df <- data.frame(
  Feature = rownames(rf_model$importance)[sorted_indices],
  Importance = feature_importances[sorted_indices]
)

# Plot feature importances
ggplot(importances_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "blue") +
  coord_flip() +  # Flip the coordinates for better readability
  labs(title = "Feature Importances (Random Forest)", x = "Features", y = "Importance") +
  theme_minimal()
```

